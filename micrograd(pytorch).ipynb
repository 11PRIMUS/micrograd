{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679bac5b",
   "metadata": {},
   "source": [
    "## exact same with Pytorch\n",
    "\n",
    "#### Happenings below:\n",
    "\n",
    "- We are defining the variables as `requires_grad=True` so that PyTorch knows that it needs to calculate the gradients for these variables. It's set to `False` by default.\n",
    "- We are defining the operations as `torch` operations.\n",
    "- The `.double()` method is used to convert the variables to `float64`. This is because Python uses `float64` by default, but the default element datatype for `torch.Tensor()` is `float32`. We want to make sure that we are comparing apples to apples.\n",
    "- The `.item()` method is used to get the actual value of the tensor. This is because the tensor itself is a wrapper around the actual value, and we need to get the actual value to compare it with our implementation.\n",
    "- Torch already has its own `backward()` function, so we don't need to implement it ourselves. We can just call `loss.backward()` and it will calculate the gradients for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84315ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: 0.7070699720278941\n",
      "x2: 0.5000520546564731\n",
      "w2: 0.0\n",
      "x1: -1.5001561639694192\n",
      "w1: 1.0001041093129461\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double();x1.requires_grad=True\n",
    "x2 = torch.Tensor([0.0]).double();x2.requires_grad=True\n",
    "w1 = torch.Tensor([-3.0]).double();w1.requires_grad=True\n",
    "w2 = torch.Tensor([1.0]).double();w2.requires_grad=True\n",
    "b=torch.Tensor([6.8813]).double();b.requires_grad=True\n",
    "\n",
    "n=x1*w1 + x2*w2 +b\n",
    "o=torch.tanh(n)\n",
    "\n",
    "print('o:', o.item())\n",
    "o.backward()\n",
    "\n",
    "print('x2:', x2.grad.item())\n",
    "print('w2:', w2.grad.item())\n",
    "print('x1:', x1.grad.item())\n",
    "print('w1:', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43a3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data * other.data)\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data + other.data)\n",
    "    def tanh(self):\n",
    "        import math\n",
    "        return Value((math.exp(2*self.data)-1)/(math.exp(2*self.data)+1))\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70fb0b",
   "metadata": {},
   "source": [
    "## Building a MLP in Micrograd\n",
    "\n",
    "### satrting with a single individual neuron\n",
    "\n",
    "#### index\n",
    "- nin =number of inputs\n",
    "- nout =number of outputs\n",
    "- w = weights\n",
    "- b = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a418f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.29220264597018003)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w= [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b= Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x):\n",
    "        #w*x+b\n",
    "        act=sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out= act.tanh()\n",
    "        return out\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, nin,nout):\n",
    "        self.neurons=[Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self,x):\n",
    "        outs=[n(x) for n in self.neurons]\n",
    "        return outs\n",
    "\n",
    "class MLP:  # instead of single input taking list of inputs, the list defines the size of all layer in our mlp\n",
    "    def __init__(self, nin , nouts):\n",
    "        sz=[nin]+nouts\n",
    "        self.layers=[Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "\n",
    "x=[2.0,3.0, -1.0]\n",
    "n=MLP(3,[4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87618faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2c8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
